# -*- coding: utf-8 -*-
"""handwritten digits project.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GJTqaLgWEongGh7xiyBIm_YSziWDcugd

## introduction: in this notebook we  are going to use different models of multiclass classification to predict handwritten digitsfrom zero to nine

## approach  followed
* problem definition
* visualizing data
* building different models with diferrent parameters
* evaluating model 
* experimentaion
* checking on a random image

## problem definition

* this dataset is to predict digits from zero to nine.
* this dataset is extracted from tensorflow datasets
"""

import tensorflow as tf

import tensorflow as tf
from tensorflow.keras.datasets import mnist
(train_data, train_labels), (test_data, test_labels) =mnist.load_data()

train_data[0].shape, train_labels[0].shape

import tensorflow_datasets as tfds
mnist_data = tfds.load("mnist")
for item in mnist_data:
  print(item)


mnist_data, info = tfds.load(name="mnist", with_info="true")
print(info)

from tensorflow.keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Commented out IPython magic to ensure Python compatibility.
# plot single sample
import matplotlib.pyplot as plt
# %matplotlib inline
plt.imshow(train_data[2]);

"""# more information about data"""

import os

x_train[0].shape

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

ds,info=tfds.load("mnist", split="train", with_info=True)
print(info.splits["train"].num_examples)
print(info.features["label"].num_classes)
ds=ds.batch(128).repeat(10)
for ex in tfds.as_numpy(ds):
  np_image, np_label=ex["image"],ex["label"]

"""* there are 60000 samples in train set with 10 different classes"""

print(f"training sample:\n {train_data[0]}\n")
print(f"training label:\n{train_labels[0]}\n")

plt.imshow(train_data[0]);

train_data[0].shape, train_labels[0].shape

train_labels[2]

class_names=["zero", "one","two", "three","four", "five","six","seven","eight","nine"]
class_names

"""## potting image using index """

index_of_choice=10 # index of choice can be changed to check different classes
plt.imshow(train_data[index_of_choice], cmap=plt.cm.binary)
plt.title(class_names[train_labels[index_of_choice]])

"""## plotting multiple images"""

#plot multiple random images of mnist
import random
plt.figure(figsize=(7,7))
for i in range(4):
  ax=plt.subplot(2,2,i+1)
  rand_index=random.choice(range(len(train_data)))
  plt.imshow(train_data[rand_index], cmap=plt.cm.binary)
  plt.title(class_names[train_labels[rand_index]])
  plt.axis(False)

"""## building a multi class model"""

flatten_model=tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(28,28))])
flatten_model.output_shape

28*28

train_labels

len(class_names)

"""## model_1"""

# set random seed
tf.random.set_seed(42)
model_1=tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28,28)),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(10, activation="softmax")
])
# compile model
model_1.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                 optimizer=tf.keras.optimizers.SGD(),
                 metrics=["accuracy"])
# fit model
non_norm_history_1=model_1.fit(train_data,
                              train_labels,
                              epochs=10,
                              validation_data=(test_data, test_labels))

"""* as the accuracy of validation set is very low, should try another model using different parameters"""

x_train.shape, y_train.shape, x_test.shape, y_test.shape

model_1.evaluate(x_test,y_test)

model_1.summary()

def plot_loss_curves(history):
  """
  returns separate loss curves for training and vallidation metrics
  """
  loss=history.history["loss"]
  val_loss=history.history["val_loss"]
  accuracy=history.history["accuracy"]
  val_accuracy=history.history["val_accuracy"]
  epochs=range(len(history.history["loss"])) # how many epochs did run 
  # plot loss
  plt.plot(epochs,loss, label="training_loss")
  plt.plot(epochs,val_loss, label="val_loss")
  plt.title("loss")
  plt.xlabel("epochs")
  plt.legend()
  # plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label="training_accuracy")
  plt.plot(epochs, val_accuracy, label="val_accuracy")
  plt.title("accuracy")
  plt.xlabel("epochs")
  plt.legend()

plot_loss_curves(non_norm_history_1)

"""* observation: model_1 is not the best model as it has accuracy of just 11% and validation loss is also very low

* normalized data
"""

# can get training and testing data between 0 and 1 by dividing by maximum 
train_data_norm=train_data/255.0
test_data_norm=test_data/255.0
# check min and max value of scaled of training data
train_data_norm.min(), train_data_norm.max()

"""## model_2 (normalized data) 
* changed optimizer to Adam 
"""

tf.random.set_seed(42)
model_2=tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28,28)),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(10, activation="softmax")
])
# compile model
model_2.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                 optimizer=tf.keras.optimizers.Adam(),
                 metrics=["accuracy"])
# fit model
norm_history_2=model_2.fit(train_data,
                              train_labels,
                              epochs=10,
                              validation_data=(test_data, test_labels))

model_2.summary()

model_2.evaluate(x_test,y_test)

plot_loss_curves(norm_history_2)

train_data.min(),train_data.max()

import pandas as pd
# plot loss curves
pd.DataFrame(non_norm_history_1.history).plot(title="non normalized data")
pd.DataFrame(norm_history_2.history).plot(title="normalized data");

#  check out another way of viewing deep learning models
from tensorflow.keras.utils import plot_model
# see inputs and outputs of each layer
plot_model(model_2,show_shapes=True)

"""## model_3
* increased layers
"""

tf.random.set_seed(42)
model_3=tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28,28)),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(10, activation="softmax")
])
# compile model
model_3.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                 optimizer=tf.keras.optimizers.Adam(),
                 metrics=["accuracy"])
# fit model
norm_history_3=model_3.fit(train_data,
                              train_labels,
                              epochs=10,
                              validation_data=(test_data, test_labels))

model_3.evaluate(x_test,y_test)

plot_loss_curves(norm_history_3)

"""## model_4
* increased epochs to 20
"""

tf.random.set_seed(42)
model_4=tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28,28)),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(10, activation="softmax")
])
# compile model
model_4.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                 optimizer=tf.keras.optimizers.Adam(),
                 metrics=["accuracy"])
# fit model
norm_history_4=model_4.fit(train_data,
                              train_labels,
                              epochs=20,
                              validation_data=(test_data, test_labels))

model_4.evaluate(x_test,y_test)

plot_loss_curves(norm_history_4)

"""## model_5
* increased number of layers, epochs and set a callback
"""

tf.random.set_seed(42)
model_5=tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28,28)),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(10, activation="softmax")
])
model_5.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                 optimizer=tf.keras.optimizers.Adam(),
                 metrics=["accuracy"])
# create learning rate callback
lr_scheduler=tf.keras.callbacks.LearningRateScheduler(lambda epoch:1e-3*10**(epoch/20))
find_lr_history=model_5.fit(train_data_norm,
                             train_labels,
                             epochs=40,
                             validation_data=(test_data_norm, test_labels),
                             callbacks=[lr_scheduler])

plot_loss_curves(find_lr_history)

model_5.evaluate(x_test,y_test)

"""## learning rate decay curve"""

# Commented out IPython magic to ensure Python compatibility.
# plot learning rate decay curve
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
lrs=1e-3*(10**(tf.range(40)/20))
plt.semilogx(lrs, find_lr_history.history["loss"])
plt.xlabel("learning rate")
plt.ylabel("loss")
plt.title("finding ideal learning rate");

"""## model_6
* increased epochs, added learning rate
"""

tf.random.set_seed(42)
model_6=tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28,28)),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(4,activation="relu"),
  tf.keras.layers.Dense(10, activation="softmax")
])
model_6.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                 metrics=["accuracy"])
history_6=model_6.fit(train_data_norm,
                        train_labels,
                        epochs=60,
                        validation_data=(test_data_norm, test_labels))

plot_loss_curves(history_6)

"""# checking random images """

import random
def plot_random_image(model, images, true_labels, classes):
  """
  picks a random image, plots it and labels it with a prediction and truth label
  """
  # set up randomm integer
  i=random.randint(0, len(images))
  # create predictions and targets
  target_image =images[i]
  pred_probs=model.predict(target_image.reshape(1,28,28))
  pred_label=classes[pred_probs.argmax()]
  true_label=classes[true_labels[i]]
  # plot image
  plt.imshow(target_image,cmap=plt.cm.binary)
  # change color of titles depending on if prediction is right or wrong
  if pred_label==true_label:
    color="green"
  else:
    color="red"
  # add x information(prediction/true label)
  plt.xlabel("pred:{}{:2.0f}% (True:{})".format(pred_label,
                                                100*tf.reduce_max(pred_probs),
                                                true_label),
             color=color)

"""* model_1 predictions"""

# check out random image as well as prediction
plot_random_image(model=model_1,
                  images=test_data_norm,
                  true_labels=test_labels,
                  classes=class_names)

"""* model_2 predictions"""

# check out random image as well as prediction
plot_random_image(model=model_2,
                  images=test_data_norm, # always make predictions on same kind of data model was trained on
                  true_labels=test_labels,
                  classes=class_names)

"""* mdoel_3 predictions"""

# check out random image as well as prediction
plot_random_image(model=model_3,
                  images=test_data_norm, # always make predictions on same kind of data model was trained on
                  true_labels=test_labels,
                  classes=class_names)

"""* model_4 predictions"""

# check out random image as well as prediction
plot_random_image(model=model_4,
                  images=test_data_norm, # always make predictions on same kind of data model was trained on
                  true_labels=test_labels,
                  classes=class_names)

"""* model_5 predictions"""

# check out random image as well as prediction
plot_random_image(model=model_5,
                  images=test_data_norm, # always make predictions on same kind of data model was trained on
                  true_labels=test_labels,
                  classes=class_names)

"""* model_6 predictions"""

# check out random image as well as prediction
plot_random_image(model=model_6,
                  images=test_data_norm, # always make predictions on same kind of data model was trained on
                  true_labels=test_labels,
                  classes=class_names)

"""## saving a model"""

model_6.save("model_6_saved")

# load in trained model and evaluate it
loaded_model_6=tf.keras.models.load_model("model_6_saved")

